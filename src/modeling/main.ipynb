{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data mining TorontoFireIncidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customs Modules\n",
    "\n",
    "- data_clean\n",
    "\n",
    "  Classes:\n",
    "  \n",
    "  - `DataCleaner`:\n",
    "  \n",
    "    **Functions**:\n",
    "    \n",
    "    - `createPipeline()`: Returns a pipeline with an imputer.\n",
    "    - `cleanse_dataframe()`: Returns a cleansed dataframe.\n",
    "    \n",
    "---\n",
    "- data_reduction\n",
    "\n",
    "  Classes:\n",
    "  \n",
    "  - `FeatureAnalysis`:\n",
    "  \n",
    "    **Functions**:\n",
    "    \n",
    "    - `keepStrongestFeaturesInDataFrame(responseVariable, df)`: Returns a dataframe with variables that have a strong correlation to `responseVariable`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Pipeline\n",
    "\n",
    "The pipeline is designed as follows:\n",
    "\n",
    "- `Pipeline`\n",
    "  - `Preprocessor`\n",
    "    - `Data_cleaning (Ryan)`\n",
    "      - Drop Null rows (inside data_clean.cleanse_dataframe() method)\n",
    "      - Drop False positives (inside data_clean.cleanse_dataframe() method)\n",
    "      - Impute missing values\n",
    "      - Remove outliers (inside data_clean.cleanse_dataframe() method)\n",
    "    - `Data_reduction (Ryan)` (mostly exists in data_reduction module outside sklearn pipeline)\n",
    "      - select best predictors (identify the variables which have a strong correlation with the response variable)\n",
    "      - identify the variables which have a strong correlation with the response variable: Kruskal-Wallis Test, Spearman coefficient, Chi-Squared (Χ²) Test\n",
    "    - `Feature_Engineering` (may exist inside a helper function outside the sklearn pipeline)\n",
    "      - create a new feature Control Time. (how long fire burned for)\n",
    "      - create a new feature called response time (how long took the first arriving unit to incident.)\n",
    "    - `feature_transformers`\n",
    "      - categorical one hot encoding\n",
    "      - categorical ordinal encoding\n",
    "      - numerical scaler\n",
    "      - log transformation on response variable\n",
    "      - normalize features\n",
    "  - `model(regressor)` \n",
    "    - Linear models\n",
    "      - Multiple Linear Regression (OLS - Ordinary Least Squares): `(Maurilio)`\n",
    "      - Lasso (Least Absolute Shrinkage and Selection Operator): `(Maurilio)`\n",
    "      - Elastic-Net: `(Maurilio)`\n",
    "      - Huber Regressor:\n",
    "    - Ensemble methods\n",
    "      - XGBoost Regressor\n",
    "    - Non-Linear Models\n",
    "      - Neural networks (MLP - Multi-layer Perceptron):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third Party libraries\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DF\n",
    "df = pd.read_csv('../../data/raw/Fire_Incidents_Data.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "The following data will be removed:\n",
    "- False positives (Final_Incident_Type: 03 - NO LOSS OUTDOOR fire (exc: Sus.arson,vandal,child playing,recycling, or dump fires)\n",
    "- Null Values for null values for Estimated Loss (response variable) or Area_of_Origin \n",
    "\n",
    "Missing Data will be imputed using KNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataCleaner.createPipeline() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m DataCleaner\u001b[38;5;241m.\u001b[39mcleanse_dataframe(df)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Data_cleaning pipeline (contains imputer)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m data_cleaning \u001b[38;5;241m=\u001b[39m \u001b[43mDataCleaner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreatePipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: DataCleaner.createPipeline() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "# Import Data_cleaning module\n",
    "from modules.data_clean import DataCleaner\n",
    "\n",
    "# Cleanse Dataframe\n",
    "df = DataCleaner.cleanse_dataframe(df)\n",
    "\n",
    "# Data_cleaning pipeline (contains imputer)\n",
    "data_cleaning = DataCleaner.createPipeline() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Reduction\n",
    "Data reduction will be focused on selecting the best predictors to use in our model.\n",
    "\n",
    "Applying correlation analysis, we will identify the variables which have a strong correlation with the response variable: Kruskal-Wallis Test, Spearman coefficient, Chi-Squared (Χ²) Test will be utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data_reduction module\n",
    "from modules.data_reduction import FeatureAnalysis\n",
    "\n",
    "# helper function that will drop low correlated variables in the dataset\n",
    "df = FeatureAnalysis.keepStrongestFeaturesInDataFrame('estimated_loss', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_engineering pipeline\n",
    "feature_engineering = Pipeline([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_transformers pipeline\n",
    "feature_transformers = Pipeline([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessor pipeline\n",
    "preprocessor = Pipeline(steps=[('data cleaning', data_cleaning),\n",
    "                               #('data reduction', data_reduction),\n",
    "                               ('feature engineering', feature_engineering)\n",
    "                               ('feature transformers', feature_transformers),\n",
    "                                ])\n",
    "# sample model\n",
    "model = KNeighborsClassifier(n_neighbors=3)  \n",
    "\n",
    "# Assemble final pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('regressor', model)])\n",
    "\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Print Results\n",
    "\n",
    "def print_results(r2, mae, mse, coefficients, intercept):\n",
    "    print(f\"R-squared score: {r2:.4f}\")\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Coefficients: {coefficients}\")\n",
    "    print(f\"Intercept: {intercept}\")\n",
    "\n",
    "# Function: Return Results\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "def get_results(model, y_test, y_pred):\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    coefficients = model.coef_\n",
    "    intercept = model.intercept_\n",
    "    return (r2, mae, mse, coefficients, intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12) # random_state should be 12 for all models\n",
    "\n",
    "lr = LinearRegression() # to do - settings hyperparameters \n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "residuals_lr = y_test - y_pred_lr\n",
    "\n",
    "(r2_lr, mae_lr, mse_lr, coefficients_lr, intercept_lr) = get_results(lr, y_test, y_pred_lr)\n",
    "\n",
    "# Results - Another option would be to use statsmodels to display a summary\n",
    "print(\"---------MLR----------\")\n",
    "print_results(r2_lr, mae_lr, mse_lr, coefficients_lr, intercept_lr)\n",
    "print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Need to scale before using Lasso. I am not sure if we've already done in preprocessing\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "# Lasso Model 1 = basic\n",
    "\n",
    "lasso = Lasso()\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "\n",
    "(r2_lasso, mae_lasso, mse_lasso, coefficients_lasso, intercept_lasso) = get_results(lasso, y_test, y_pred_lasso)\n",
    "\n",
    "\n",
    "print(\"---------LASSO----------\")\n",
    "print_results(r2_lasso, mae_lasso, mse_lasso, coefficients_lasso, intercept_lasso)\n",
    "print(\"------------------------\")\n",
    "\n",
    "# Lasso Model 2 = Testing different parameters (CROSS-VALIDATOR: CV)\n",
    "\n",
    "param_grid = {\n",
    "\t'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\t\n",
    "}\n",
    "\n",
    "lasso_cv = GridSearchCV(lasso, param_grid, cv=3, n_jobs=-1)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lasso_cv = lasso_cv.predict(X_test)\n",
    "\n",
    "(r2_lasso_cv, mae_lasso_cv, mse_lasso_cv, coefficients_lasso_cv, intercept_lasso_cv) = get_results(lasso_cv, y_test, y_pred_lasso_cv)\n",
    "\n",
    "print(\"---------LASSO CV----------\")\n",
    "print_results(r2_lasso_cv, mae_lasso_cv, mse_lasso_cv, coefficients_lasso_cv, intercept_lasso_cv)\n",
    "print(\"---------------------------\")\n",
    "\n",
    "# save best alpha paramter\n",
    "best_alpha = lasso_cv.best_estimator_\n",
    "\n",
    "# Lasso Model 3 = Lasso model with the best paramters\n",
    "lasso_best = Lasso(alpha=best_alpha)\n",
    "lasso_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# X_train and X_test should be scaled\n",
    "\n",
    "# ElasticNet Model 1 = basic\n",
    "\n",
    "elastic_net = ElasticNet()\n",
    "elastic_net.fit(X_train, y_train)\n",
    "\n",
    "y_pred_elastic_net = elastic_net.predict(X_test)\n",
    "\n",
    "(r2_elastic_net, mae_elastic_net, mse_elastic_net, coefficients_elastic_net, intercept_elastic_net) = get_results(elastic_net, y_test, y_pred_elastic_net)\n",
    "\n",
    "print(\"---------ELASTIC NET BASIC----------\")\n",
    "print_results(r2_elastic_net, mae_elastic_net, mse_elastic_net, coefficients_elastic_net, intercept_elastic_net)\n",
    "print(\"---------------------------\")\n",
    "\n",
    "# ElasticNet Model 2 = Testing different parameters (CROSS-VALIDATOR: CV)\n",
    "\n",
    "param_grid = {\n",
    "\t'alpha': [0.1, 0.3, 0.5, 0.7, 0.9, 1.0],\n",
    "\t'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9, 1.0], \t\n",
    "}\n",
    "\n",
    "elastic_cv = GridSearchCV(elastic_net, param_grid, scoring='neg_mean_squared_error', cv=3, n_jobs=-1)\n",
    "elastic_cv.fit(X_train, y_train)\n",
    "\n",
    "y_pred_elastic_cv = elastic_cv.predict(X_test)\n",
    "(r2_elastic_cv, mae_elastic_cv, mse_elastic_cv, coefficients_elastic_cv, intercept_elastic_cv) = get_results(elastic_cv, y_test, y_pred_elastic_cv)\n",
    "\n",
    "print(\"---------ELASTIC NET CV----------\")\n",
    "print_results(r2_elastic_cv, mae_elastic_cv, mse_elastic_cv, coefficients_elastic_cv, intercept_elastic_cv)\n",
    "print(\"---------------------------\")\n",
    "\n",
    "\n",
    "# Best parameters\n",
    "best_estimator = elastic_cv.best_estimator_\n",
    "print(best_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample imports\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.preprocessing import OrdinalEncoder\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.impute import KNNImputer\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# import pandas as pd\n",
    "\n",
    "# df = load_data('data/loan.csv')\n",
    "\n",
    "# # Encode the target variable using LabelEncoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# df['Loan_Status'] = label_encoder.fit_transform(df['Loan_Status'])\n",
    "\n",
    "\n",
    "# # Define categorical and numerical features\n",
    "# ordinal_categorical_features = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed']\n",
    "# c1_idx = [df.columns.get_loc(item) for item in ordinal_categorical_features]\n",
    "# onehot_categorical_features = ['Property_Area']\n",
    "# c2_idx = [df.columns.get_loc(item) for item in onehot_categorical_features]\n",
    "# numerical_features = df.columns.difference(ordinal_categorical_features + onehot_categorical_features + ['Loan_Status'])\n",
    "# n_idx = [df.columns.get_loc(item) for item in numerical_features]\n",
    "\n",
    "# # Create transformers for numerical and categorical features\n",
    "# numerical_transformer = Pipeline(steps=[\n",
    "#     ('scaler', StandardScaler())\n",
    "# ])\n",
    "\n",
    "# ordinal_categorical_transformer = Pipeline(steps=[\n",
    "#     ('ordinal', OrdinalEncoder(handle_unknown='error'))\n",
    "# ])\n",
    "\n",
    "# onehot_categorical_transformer = Pipeline(steps=[\n",
    "#     ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "# ])\n",
    "\n",
    "# column_imputer = Pipeline(steps=[\n",
    "#     ('imputer0', KNNImputer())\n",
    "# ])\n",
    "\n",
    "# # Apply transformers to features using ColumnTransformer\n",
    "# feature_transformer = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('cat1', ordinal_categorical_transformer, c1_idx),\n",
    "#         ('cat2', onehot_categorical_transformer, c2_idx),\n",
    "#         ('num', numerical_transformer, n_idx),\n",
    "#     ])\n",
    "\n",
    "# missing_value_imputer = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('imputer', column_imputer, c1_idx + c2_idx + n_idx)\n",
    "#     ])\n",
    "\n",
    "\n",
    "# # Define the KNN model\n",
    "# knn_model = KNeighborsClassifier(n_neighbors=3)  # You can adjust the number of neighbors\n",
    "\n",
    "# # Create the pipeline\n",
    "# # Create preprocessing and training pipeline\n",
    "# pipeline = Pipeline(steps=[('transformer', feature_transformer),\n",
    "#                            ('imputer', missing_value_imputer),\n",
    "#                            ('classifier', knn_model)])\n",
    "# pipeline\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
