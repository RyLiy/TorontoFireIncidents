{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data mining TorontoFireIncidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customs Modules\n",
    "\n",
    "- data_clean\n",
    "\n",
    "  Classes:\n",
    "  \n",
    "  - `DataCleaner`:\n",
    "  \n",
    "    **Functions**:\n",
    "    \n",
    "    - `createPipeline()`: Returns a pipeline with an imputer.\n",
    "    - `cleanse_dataframe()`: Returns a cleansed dataframe.\n",
    "    \n",
    "---\n",
    "- data_reduction\n",
    "\n",
    "  Classes:\n",
    "  \n",
    "  - `FeatureAnalysis`:\n",
    "  \n",
    "    **Functions**:\n",
    "    \n",
    "    - `keepStrongestFeaturesInDataFrame(responseVariable, df)`: Returns a dataframe with variables that have a strong correlation to `responseVariable`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Pipeline\n",
    "\n",
    "The pipeline is designed as follows:\n",
    "\n",
    "- `Pipeline`\n",
    "  - `Preprocessor`\n",
    "    - `Data_cleaning (Ryan)`\n",
    "      - Drop Null rows (inside data_clean.cleanse_dataframe() method)\n",
    "      - Drop False positives (inside data_clean.cleanse_dataframe() method)\n",
    "      - Impute missing values\n",
    "      - Remove outliers (inside data_clean.cleanse_dataframe() method)\n",
    "    - `Data_reduction (Ryan)` (mostly exists in data_reduction module outside sklearn pipeline)\n",
    "      - select best predictors (identify the variables which have a strong correlation with the response variable)\n",
    "      - identify the variables which have a strong correlation with the response variable: Kruskal-Wallis Test, Spearman coefficient, Chi-Squared (Χ²) Test\n",
    "    - `Feature_Engineering` (may exist inside a helper function outside the sklearn pipeline)\n",
    "      - create a new feature Control Time. (how long fire burned for)\n",
    "      - create a new feature called response time (how long took the first arriving unit to incident.)\n",
    "    - `feature_transformers`\n",
    "      - categorical one hot encoding\n",
    "      - categorical ordinal encoding\n",
    "      - numerical scaler\n",
    "      - log transformation on response variable\n",
    "      - normalize features\n",
    "  - `model(regressor)` \n",
    "    - Linear models\n",
    "      - Multiple Linear Regression (OLS - Ordinary Least Squares):\n",
    "      - Lasso (Least Absolute Shrinkage and Selection Operator):\n",
    "      - Elastic-Net:\n",
    "      - Huber Regressor:\n",
    "    - Ensemble methods\n",
    "      - XGBoost Regressor\n",
    "    - Non-Linear Models\n",
    "      - Neural networks (MLP - Multi-layer Perceptron):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third Party libraries\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DF\n",
    "df = pd.read_csv('../../data/raw/Fire_Incidents_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "The following data will be removed:\n",
    "- False positives (Final_Incident_Type: 03 - NO LOSS OUTDOOR fire (exc: Sus.arson,vandal,child playing,recycling, or dump fires)\n",
    "- Null Values for null values for Estimated Loss (response variable) or Area_of_Origin \n",
    "\n",
    "Missing Data will be imputed using KNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data_cleaning module\n",
    "from modules.data_clean import DataCleaner\n",
    "\n",
    "# Cleanse Dataframe\n",
    "df = DataCleaner.cleanse_dataframe(df)\n",
    "\n",
    "# Data_cleaning pipeline (contains imputer)\n",
    "data_cleaning = DataCleaner.createPipeline() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Reduction\n",
    "Data reduction will be focused on selecting the best predictors to use in our model.\n",
    "\n",
    "Applying correlation analysis, we will identify the variables which have a strong correlation with the response variable: Kruskal-Wallis Test, Spearman coefficient, Chi-Squared (Χ²) Test will be utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data_reduction module\n",
    "from modules.data_reduction import FeatureAnalysis\n",
    "\n",
    "# helper function that will drop low correlated variables in the dataset\n",
    "df = FeatureAnalysis.keepStrongestFeaturesInDataFrame('estimated_loss', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_engineering pipeline\n",
    "feature_engineering = Pipeline([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_transformers pipeline\n",
    "feature_transformers = Pipeline([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessor pipeline\n",
    "preprocessor = Pipeline(steps=[('data cleaning', data_cleaning),\n",
    "                               #('data reduction', data_reduction),\n",
    "                               ('feature engineering', feature_engineering)\n",
    "                               ('feature transformers', feature_transformers),\n",
    "                                ])\n",
    "# sample model\n",
    "model = KNeighborsClassifier(n_neighbors=3)  \n",
    "\n",
    "# Assemble final pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('regressor', model)])\n",
    "\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample imports\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.preprocessing import OrdinalEncoder\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.impute import KNNImputer\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# import pandas as pd\n",
    "\n",
    "# df = load_data('data/loan.csv')\n",
    "\n",
    "# # Encode the target variable using LabelEncoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# df['Loan_Status'] = label_encoder.fit_transform(df['Loan_Status'])\n",
    "\n",
    "\n",
    "# # Define categorical and numerical features\n",
    "# ordinal_categorical_features = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed']\n",
    "# c1_idx = [df.columns.get_loc(item) for item in ordinal_categorical_features]\n",
    "# onehot_categorical_features = ['Property_Area']\n",
    "# c2_idx = [df.columns.get_loc(item) for item in onehot_categorical_features]\n",
    "# numerical_features = df.columns.difference(ordinal_categorical_features + onehot_categorical_features + ['Loan_Status'])\n",
    "# n_idx = [df.columns.get_loc(item) for item in numerical_features]\n",
    "\n",
    "# # Create transformers for numerical and categorical features\n",
    "# numerical_transformer = Pipeline(steps=[\n",
    "#     ('scaler', StandardScaler())\n",
    "# ])\n",
    "\n",
    "# ordinal_categorical_transformer = Pipeline(steps=[\n",
    "#     ('ordinal', OrdinalEncoder(handle_unknown='error'))\n",
    "# ])\n",
    "\n",
    "# onehot_categorical_transformer = Pipeline(steps=[\n",
    "#     ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "# ])\n",
    "\n",
    "# column_imputer = Pipeline(steps=[\n",
    "#     ('imputer0', KNNImputer())\n",
    "# ])\n",
    "\n",
    "# # Apply transformers to features using ColumnTransformer\n",
    "# feature_transformer = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('cat1', ordinal_categorical_transformer, c1_idx),\n",
    "#         ('cat2', onehot_categorical_transformer, c2_idx),\n",
    "#         ('num', numerical_transformer, n_idx),\n",
    "#     ])\n",
    "\n",
    "# missing_value_imputer = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('imputer', column_imputer, c1_idx + c2_idx + n_idx)\n",
    "#     ])\n",
    "\n",
    "\n",
    "# # Define the KNN model\n",
    "# knn_model = KNeighborsClassifier(n_neighbors=3)  # You can adjust the number of neighbors\n",
    "\n",
    "# # Create the pipeline\n",
    "# # Create preprocessing and training pipeline\n",
    "# pipeline = Pipeline(steps=[('transformer', feature_transformer),\n",
    "#                            ('imputer', missing_value_imputer),\n",
    "#                            ('classifier', knn_model)])\n",
    "# pipeline\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
